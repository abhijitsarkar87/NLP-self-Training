{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f79ba210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing nltk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7895fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install nltk\n",
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f7c34",
   "metadata": {},
   "source": [
    "\"NOTE\": if found Error Please Install and import libraries:\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"all\") | nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba715d45",
   "metadata": {},
   "source": [
    "# Tokenization:\n",
    "\n",
    "**Tokenization** is a process of **breaking down a given paragraph of text into a list of sentences or words**. \n",
    "1. When a paragraph is broken down into sentences it is known as **sentence tokenization**. \n",
    "2. When a sentence is broken up into words its known as **word tokenization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79d739b",
   "metadata": {},
   "source": [
    "### Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "661d7f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'Learning', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "# import word tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "Words=\"I am Learning NLP\"\n",
    "print(word_tokenize(Words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee87a4e",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bd011b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We are Learning NLP.', 'I got 100%.', 'out of 100']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import library\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentence=\"We are Learning NLP. I got 100%. out of 100\"\n",
    "sent_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5e043af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Bussiness annual growth rate is 25.50%.\n",
      "Good job Mr. Bajaj\n"
     ]
    }
   ],
   "source": [
    "text=\"Our Bussiness annual growth rate is 25.50%. Good job Mr. Bajaj\"\n",
    "for i in sent_tokenize(text):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a508a",
   "metadata": {},
   "source": [
    "## Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6ea6873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP is fun and can deal with texts and sounds, but can't deal with images. We have session at 12AM!. We can earn lot of $\n"
     ]
    }
   ],
   "source": [
    "# import libray\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "# sample text\n",
    "text=\"NLP is fun and can deal with texts and sounds, but can't deal with images. We have session at 12AM!. We can earn lot of $\"\n",
    "\n",
    "# print text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12f4f905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'fun', 'and', 'can', 'deal', 'with', 'texts', 'and', 'sounds', 'but', \"can't\", 'deal', 'with', 'images', 'e', 'have', 'session', 'at', 'e', 'can', 'earn', 'lot', 'of']\n"
     ]
    }
   ],
   "source": [
    "# print word by word only small case and starts from small a to z\n",
    "print(regexp_tokenize(text,\"['a-z']+\"))\n",
    "\n",
    "# We can filter small letters A to Z in lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c095aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'fun', 'and', 'can', 'deal', 'with', 'texts', 'and', 'sounds', 'but', \"can't\", 'deal', 'with', 'images', 'e', 'have', 'session', 'at', 'e', 'can', 'earn', 'lot', 'of']\n"
     ]
    }
   ],
   "source": [
    "# print extra quote.\n",
    "print(regexp_tokenize(text,\"[a-z']+\")) \n",
    "\n",
    "# will filter small letter and semi-colon from columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31b4e78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'W', 'AM', 'W']\n"
     ]
    }
   ],
   "source": [
    "# only caps items from the paragraph\n",
    "print(regexp_tokenize(text,\"[A-Z]+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83c1cdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'is', 'fun', 'and', 'can', 'deal', 'with', 'texts', 'and', 'sounds', 'but', \"can't\", 'deal', 'with', 'images', 'We', 'have', 'session', 'at', 'AM', 'We', 'can', 'earn', 'lot', 'of']\n"
     ]
    }
   ],
   "source": [
    "# all items to be printed in oneline\n",
    "print(regexp_tokenize(text,\"[/aA-zZ']+\"))\n",
    "\n",
    "# will filter all alpha values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5b2ea23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ', ', ' ', ' ', ' ', ' ', '. W', ' ', ' ', ' ', ' 12AM!. W', ' ', ' ', ' ', ' ', ' $']\n"
     ]
    }
   ],
   "source": [
    "# anything starts with caret is not equal to the following\n",
    "print(regexp_tokenize(text,\"[^a-z']+\"))\n",
    "\n",
    "# will ignore all small letter a to z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48915f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12']\n"
     ]
    }
   ],
   "source": [
    "# show numbers\n",
    "print(regexp_tokenize(text,\"[/0-9]+\"))\n",
    "\n",
    "# will show only numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b10da5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"NLP is fun and can deal with texts and sounds, but can't deal with images. We have session at \", 'AM!. We can earn lot of $']\n"
     ]
    }
   ],
   "source": [
    "# remove number\n",
    "print(regexp_tokenize(text,\"[^0-9]+\"))\n",
    "\n",
    "# ignore only numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37aed5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"NLP is fun and can deal with texts and sounds, but can't deal with images. We have session at 12AM!. We can earn lot of \"]\n"
     ]
    }
   ],
   "source": [
    "# remove symbols\n",
    "print(regexp_tokenize(text,\"[^$]+\"))\n",
    "\n",
    "# ignore symbol $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b77191c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show symbol\n",
    "regexp_tokenize(text,\"[/$]\")\n",
    "\n",
    "# will show only doller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbe9846",
   "metadata": {},
   "source": [
    "# STOP WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a0c7eb",
   "metadata": {},
   "source": [
    "The **stop words are the common words** and **are very common in occurance such as \"a\",\"an\",\"the\",\"etc\"**. We ignore such words during the preprocessing part since they **don't give any important information and would just take additional space.**\n",
    "\n",
    "- import nltk.corpus import stopwords #import stopwords\n",
    "\n",
    "**Download:** nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b1ca568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stopwords\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0644cd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# checking english stop words\n",
    "stop_words=stopwords.words(\"english\")\n",
    "\n",
    "#print stop words\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca4a0f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length stop words\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc36e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra ways to pull stop words\n",
    "stopset=nltk.corpus.stopwords.words(\"English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31d65851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "LENGTH:  179\n"
     ]
    }
   ],
   "source": [
    "print(stopset)\n",
    "print(\"\\nLENGTH: \",len(stopset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6decd6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# added a new stop word\n",
    "stopset.append(\"+ve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3261e8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3b78769",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopset.extend(\"$,**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23b82f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '+ve', '$', ',', '*', '*']\n"
     ]
    }
   ],
   "source": [
    "print(stopset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "576d99c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58e66651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2e31968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance of punctuation\n",
    "punc_box=string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70b89428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96a9adb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "# making instance of stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_w=stopwords.words(\"english\")\n",
    "print(len(stop_w))\n",
    "\n",
    "# or\n",
    "\n",
    "stop_wrds=nltk.corpus.stopwords.words(\"english\")\n",
    "print(len(stop_wrds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ae57f2",
   "metadata": {},
   "source": [
    "# How to Remove stop words and Punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64cb6a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'Bhārat', the name for India in several Indian languages, is mainly derived from the name of the Vedic community of Bharatas who are mentioned in the Rigveda as one of the principal kingdoms of the Aryavarta. It is also variously said to be derived from the name of either Dushyanta's son Bharata or Mahabharata.[1] At first the name Bhārat referred only to the western part of the Gangetic Valley,[2][3] but was later more broadly applied to the Indian subcontinent and the region of Greater India, as was the name 'India'. Today it refers to the contemporary Republic of India located therein. The name 'India' is originally derived from the name of the river Sindhu (Indus River) and has been in use in Greek since Herodotus (5th century BCE). The term appeared in Old English as early the 9th century and reemerged in Modern English in the 17th century.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para=\"'Bhārat', the name for India in several Indian languages, is mainly derived from the name of the Vedic community of Bharatas who are mentioned in the Rigveda as one of the principal kingdoms of the Aryavarta. It is also variously said to be derived from the name of either Dushyanta's son Bharata or Mahabharata.[1] At first the name Bhārat referred only to the western part of the Gangetic Valley,[2][3] but was later more broadly applied to the Indian subcontinent and the region of Greater India, as was the name 'India'. Today it refers to the contemporary Republic of India located therein. The name 'India' is originally derived from the name of the river Sindhu (Indus River) and has been in use in Greek since Herodotus (5th century BCE). The term appeared in Old English as early the 9th century and reemerged in Modern English in the 17th century.\"\n",
    "para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dfcf3479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'Bhārat\", 'name', 'India', 'several', 'Indian', 'languages', 'mainly', 'derived', 'name', 'Vedic', 'community', 'Bharatas', 'mentioned', 'Rigveda', 'one', 'principal', 'kingdoms', 'Aryavarta', 'It', 'also', 'variously', 'said', 'derived', 'name', 'either', 'Dushyanta', \"'s\", 'son', 'Bharata', 'Mahabharata', '1', 'At', 'first', 'name', 'Bhārat', 'referred', 'western', 'part', 'Gangetic', 'Valley', '2', '3', 'later', 'broadly', 'applied', 'Indian', 'subcontinent', 'region', 'Greater', 'India', 'name', \"'India\", 'Today', 'refers', 'contemporary', 'Republic', 'India', 'located', 'therein', 'The', 'name', \"'India\", 'originally', 'derived', 'name', 'river', 'Sindhu', 'Indus', 'River', 'use', 'Greek', 'since', 'Herodotus', '5th', 'century', 'BCE', 'The', 'term', 'appeared', 'Old', 'English', 'early', '9th', 'century', 'reemerged', 'Modern', 'English', '17th', 'century']\n"
     ]
    }
   ],
   "source": [
    "clean_box=[]\n",
    "\n",
    "for words in nltk.tokenize.word_tokenize(para):\n",
    "    if words not in stop_wrds: \n",
    "        if words not in punc_box:\n",
    "            clean_box.append(words)\n",
    "            \n",
    "print(clean_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "058334d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Original Para : 857\n",
      "Length of Clean_box:  89\n"
     ]
    }
   ],
   "source": [
    "# checking the difference:\n",
    "print(\"Length of Original Para :\",len(para))\n",
    "print(\"Length of Clean_box: \",len(clean_box))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47431b0e",
   "metadata": {},
   "source": [
    "# Upper CASE & Lower CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8492f8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LETTERS IN ALL UPPER CASE\n",
      "--------------------------------------------------\n",
      " 'BHĀRAT', THE NAME FOR INDIA IN SEVERAL INDIAN LANGUAGES, IS MAINLY DERIVED FROM THE NAME OF THE VEDIC COMMUNITY OF BHARATAS WHO ARE MENTIONED IN THE RIGVEDA AS ONE OF THE PRINCIPAL KINGDOMS OF THE ARYAVARTA. IT IS ALSO VARIOUSLY SAID TO BE DERIVED FROM THE NAME OF EITHER DUSHYANTA'S SON BHARATA OR MAHABHARATA.[1] AT FIRST THE NAME BHĀRAT REFERRED ONLY TO THE WESTERN PART OF THE GANGETIC VALLEY,[2][3] BUT WAS LATER MORE BROADLY APPLIED TO THE INDIAN SUBCONTINENT AND THE REGION OF GREATER INDIA, AS WAS THE NAME 'INDIA'. TODAY IT REFERS TO THE CONTEMPORARY REPUBLIC OF INDIA LOCATED THEREIN. THE NAME 'INDIA' IS ORIGINALLY DERIVED FROM THE NAME OF THE RIVER SINDHU (INDUS RIVER) AND HAS BEEN IN USE IN GREEK SINCE HERODOTUS (5TH CENTURY BCE). THE TERM APPEARED IN OLD ENGLISH AS EARLY THE 9TH CENTURY AND REEMERGED IN MODERN ENGLISH IN THE 17TH CENTURY.\n"
     ]
    }
   ],
   "source": [
    "print(\"LETTERS IN ALL UPPER CASE\\n--------------------------------------------------\\n\",\n",
    "      para.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a326f7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOWER CASE: \n",
      "--------------------------------------------------------------\n",
      " 'bhārat', the name for india in several indian languages, is mainly derived from the name of the vedic community of bharatas who are mentioned in the rigveda as one of the principal kingdoms of the aryavarta. it is also variously said to be derived from the name of either dushyanta's son bharata or mahabharata.[1] at first the name bhārat referred only to the western part of the gangetic valley,[2][3] but was later more broadly applied to the indian subcontinent and the region of greater india, as was the name 'india'. today it refers to the contemporary republic of india located therein. the name 'india' is originally derived from the name of the river sindhu (indus river) and has been in use in greek since herodotus (5th century bce). the term appeared in old english as early the 9th century and reemerged in modern english in the 17th century.\n"
     ]
    }
   ],
   "source": [
    "print(\"LOWER CASE: \\n--------------------------------------------------------------\\n\",\n",
    "      para.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913f39a5",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3405504e",
   "metadata": {},
   "source": [
    "# STEMMING ,LEMMATIZATION (*********WORD NET LEMITIZER*********)\n",
    "\n",
    "- Take us to the base word source\n",
    "\n",
    "\n",
    "### 1.  STEMMING:\n",
    "\n",
    "Stemming means mapping a group of words to the same stem by removing prefixes and suffixes without giving any value to the \"grammatical meaning\" of the stem formed after the process.\n",
    "   \n",
    "   \n",
    "     - COMPUTATION --> comput\n",
    "     - computer  ----> comput\n",
    "     - hobbies ------> hobbi\n",
    "        \n",
    "WE can See that stemming tries to bring the word back to their base word but the base word may or may not have correct grammatical Meanings:\n",
    "\n",
    "    1. Porter Stemmer\n",
    "    2. Lancaster Stemmer\n",
    "    \n",
    "### Stemming Two Type:\n",
    "\n",
    "1. Lancaster Stemmer\n",
    "2. Porter Stemmer\n",
    "3. Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65df9b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANCASTER STEMMER\n",
      "--------------------------\n",
      "hobby hobby comput comput\n",
      "**************************************** \n",
      "\n",
      "PORTER STEMMER\n",
      "------------------------------\n",
      "hobbi hobbi comput comput\n",
      "**************************************** \n",
      "\n",
      "Snowball Stemmer\n",
      "----------------------------\n",
      "hobbi hobbi comput comput\n",
      "**************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer,PorterStemmer,SnowballStemmer\n",
    "\n",
    "# create instance\n",
    "lan=LancasterStemmer()\n",
    "por=PorterStemmer()\n",
    "snow=SnowballStemmer(\"english\")\n",
    "\n",
    "#They may or may not give Meaning\n",
    "print(\"LANCASTER STEMMER\\n--------------------------\")\n",
    "print(lan.stem(\"hobby\"),lan.stem(\"hobbies\"),lan.stem(\"computer\"),lan.stem(\"Computation\"))\n",
    "print(\"*\"*40,\"\\n\")\n",
    "\n",
    "print(\"PORTER STEMMER\\n------------------------------\")\n",
    "print(por.stem(\"hobby\"),por.stem(\"hobbies\"),por.stem(\"computer\"),por.stem(\"computation\"))\n",
    "print(\"*\"*40,\"\\n\")\n",
    "\n",
    "print(\"Snowball Stemmer\\n----------------------------\")\n",
    "print(snow.stem(\"hobby\"),snow.stem(\"hobbies\"),snow.stem(\"computer\"),snow.stem(\"computation\"))\n",
    "print(\"*\"*40,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a457eafb",
   "metadata": {},
   "source": [
    "- Breaks words into smaller Form.\n",
    "\n",
    "- May give or May not Give Meaningful Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b08ca57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was going to office on my bike and I saw a very big snake infront of me\n"
     ]
    }
   ],
   "source": [
    "# Lets take an example:\n",
    "\n",
    "Sentence = \"I was going to office on my bike and I saw a very big snake infront of me\"\n",
    "print(Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d5c4261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was going to office on my bike and I saw a very big snake infront of me\n"
     ]
    }
   ],
   "source": [
    "# break the words into chunks\n",
    "word_tokens=list(nltk.tokenize.word_tokenize(Sentence))\n",
    "print(\" \".join(word_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0276814b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was going to off on my bik and i saw a very big snak infront of me\n",
      "i wa go to offic on my bike and i saw a veri big snake infront of me\n",
      "i was go to offic on my bike and i saw a veri big snake infront of me\n"
     ]
    }
   ],
   "source": [
    "for stemmer_list in (lan,por,snow): # list of stemmer\n",
    "    \n",
    "    stemm=[stemmer_list.stem(token) for token in word_tokens] # stemming words\n",
    "    \n",
    "    # .join will make it into sentence\n",
    "    print(\" \".join(stemm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64bda0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "ran\n"
     ]
    }
   ],
   "source": [
    "# one more simple example of porter\n",
    "print(por.stem(\"running\"))\n",
    "print(por.stem(\"runs\"))\n",
    "print(por.stem(\"ran\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e033240",
   "metadata": {},
   "source": [
    "********************\n",
    "**KEY POINTS**\n",
    "\n",
    "1. Lancaster Algorithm is *faster* than porter but it is more complex. Porter Stemmer is the oldest algorithm present and was the most popular to use.\n",
    "\n",
    "2. Snowball Stemmer, also known as porter2, is the updated version of the Porter Stemmer and is **currently the most popular stemming algorithm**.\n",
    "\n",
    "3. Snowball stemmer is avialable for **multiple languages as well**.\n",
    "\n",
    "************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a56c9",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "- Do the same thing as the Stemming but it keeps the meaning of the base word. The base word is known as LEMMA. \n",
    "\n",
    " We use WordNet Lemmatizer for Lemmatization in nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6fb4996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c846bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "play\n"
     ]
    }
   ],
   "source": [
    "print(lemma.lemmatize(\"running\",pos=\"v\"))\n",
    "print(lemma.lemmatize(\"ran\",pos=\"v\"))\n",
    "print(lemma.lemmatize(\"plays\",pos=\"v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86645531",
   "metadata": {},
   "source": [
    "**Its Very Complex and takes lot of time to calculate. WE can use it when the context is necessary for processing,** else stemming should be preffered.\n",
    "It completely depends on the type of problem you are trying to solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e56e397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bring', 'back', 'king', 'going', 'song', 'sing', 'back', 'nothing', 'things']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One more example using both stemming and Lemma.\n",
    "\n",
    "text=\" bring back king going song sing back nothing things\"\n",
    "\n",
    "tokenization=nltk.word_tokenize(text)\n",
    "tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a8e81b",
   "metadata": {},
   "source": [
    "## EXAMPLE: Stemming Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1583054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming for bring is bring\n",
      "Stemming for back is back\n",
      "Stemming for king is king\n",
      "Stemming for going is going\n",
      "Stemming for song is song\n",
      "Stemming for sing is sing\n",
      "Stemming for back is back\n",
      "Stemming for nothing is noth\n",
      "Stemming for things is thing\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lan=LancasterStemmer()\n",
    "\n",
    "for token in tokenization:\n",
    "    print(\"Stemming for {} is {}\".format(token,lan.stem(token)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ab7fd3",
   "metadata": {},
   "source": [
    "## EXAMPLE: Lemmatization Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81d6e237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization for *bring* is bring\n",
      "Lemmatization for *back* is back\n",
      "Lemmatization for *king* is king\n",
      "Lemmatization for *going* is going\n",
      "Lemmatization for *song* is song\n",
      "Lemmatization for *sing* is sing\n",
      "Lemmatization for *back* is back\n",
      "Lemmatization for *nothing* is nothing\n",
      "Lemmatization for *things* is thing\n"
     ]
    }
   ],
   "source": [
    "for token in tokenization:\n",
    "    print(\"Lemmatization for *{}* is {}\".format(token,lemma.lemmatize(token)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f754cc",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaaf2be",
   "metadata": {},
   "source": [
    "## WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad66e0",
   "metadata": {},
   "source": [
    "**WordNet is an NLTK corpus reader, a Lexical database for English.** Used to find the meaning of words, synonym or antonym. One can define it as a sematically oriented dictionary of English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "858236f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('active.a.14') ['active_agent', 'active', 'active_voice', 'active', 'active', 'active', 'active', 'combat-ready', 'fighting', 'active', 'active', 'participating', 'active', 'active', 'active', 'active', 'alive', 'active', 'active', 'active', 'dynamic', 'active', 'active', 'active'] ['passive_voice', 'inactive', 'passive', 'inactive', 'inactive', 'inactive', 'quiet', 'passive', 'stative', 'extinct', 'dormant', 'inactive']\n"
     ]
    }
   ],
   "source": [
    "#lets find synonyms and antonyms using python code.\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonyms=[]\n",
    "antonyms=[]\n",
    "\n",
    "for syn in wordnet.synsets(\"active\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "    \n",
    "print(syn,synonyms,antonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c54ec3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'participating', 'active', 'active_agent', 'combat-ready', 'active_voice', 'fighting', 'alive', 'dynamic'}\n"
     ]
    }
   ],
   "source": [
    "# set will show no duplicate:\n",
    "print(set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "926b2f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'passive_voice', 'inactive', 'extinct', 'dormant', 'passive', 'stative', 'quiet'}\n"
     ]
    }
   ],
   "source": [
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc9767",
   "metadata": {},
   "source": [
    "# {Word Embedding} Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b3f8f9",
   "metadata": {},
   "source": [
    "**Word Embedding Count Vectorization_Bow_Ngram**\n",
    "\n",
    "Word Embedding: Computer Don't understand any Alphabets or text. They can only understand only numbers.So text are broken up into vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9862a6c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### COUNT VECTORIZER\n",
    "\n",
    "  Count Vectorizer uses two of the following models as the base to vectorize the given words on the basis of frequency of words.\n",
    "\n",
    "**1. Bag of Words**\n",
    "\n",
    "**2. N-Gram Model**\n",
    "\n",
    "---------------------------\n",
    "\n",
    "### BAG OF WORDS MODEL\n",
    "----------------------------------\n",
    "\n",
    "BOW model is used in NLP to represent the given **text/sentence/document as a collection(bag)** of words without giving any importance to grammar or the occurance order of the words. It keeps the account of frequency of the words in the text document, which can be used as features in many models.\n",
    "\n",
    "BOW is mainly used for feature selection. The above dictionary is converted as a list with only the frequency terms there and on that basis, weights are given to the most occuring terms. But the \"STOP WORDS\" are the most frequent words that appears in raw document. Thus, having a word with high frequency count doesn't mean that the word is as important. To solve this problem.*\"TF_IDF was introduced\"*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecbbbec",
   "metadata": {},
   "source": [
    "## N_gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f58a3e0",
   "metadata": {},
   "source": [
    "As discussed that Bag of words model, BOW model doesn't keep the sequence of words in a given text,only the frequency of words matters, It doesn't take info account the context of the given sentence, or care for grammatical rules such as verb is following a proper noun in the given text. n-gram model is used in such cases to keep the context of the given text intact. \n",
    "\n",
    "N-gram is the sequence of n words from a given text/document.\n",
    "\n",
    "Where \n",
    "  \n",
    "**UNIGRAM** \n",
    "1. n=1, we call it as \"UNI_GRAM\"\n",
    "ex:[I,HAVE,A, BAG]\n",
    "  \n",
    "**BI_GRAM**\n",
    "\n",
    "2. n=2, we call it as \"BI_GRAM\"\n",
    "ex:[[I HAVE],[HAVE A],[A BAG]]\n",
    "\n",
    "\n",
    "**TRI GRAM**\n",
    "\n",
    "3. n=3, we call it as \"TRI_GRAM\"\n",
    "EX:[[I HAVE A],[HAVE A BAG]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed48f4",
   "metadata": {},
   "source": [
    "### SKIP GRAMS\n",
    "\n",
    "\n",
    "Same as N-Gram. Some words can be skipped.\n",
    "\n",
    "Example: 1-SKIP,2-GRAM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1928d41",
   "metadata": {},
   "source": [
    "### BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "984b08b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Contains:  ['bag' 'example' 'is' 'of' 'this' 'words']\n"
     ]
    }
   ],
   "source": [
    "# example of single document\n",
    "# without stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "string=[\"This is a example of bag of words!\"]\n",
    "\n",
    "# convert text into vectors using countvectorizer\n",
    "vect1=CountVectorizer()\n",
    "\n",
    "#apply transformation\n",
    "vect1.fit_transform(string)\n",
    "\n",
    "# will show the feature names\n",
    "print(\"Bag of Words Contains: \",vect1.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "32450f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 4, 'is': 2, 'example': 1, 'of': 3, 'bag': 0, 'words': 5}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# will show the index and index postion\n",
    "vect1.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b82f2",
   "metadata": {},
   "source": [
    "**FIT and Transform and predict if the word is present or not.**\n",
    "\n",
    "- This is widely used for document or subject classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "62eac833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_vect=CountVectorizer()\n",
    "c_vect.fit(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "46d55487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text Present at  [[0 0 2 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "string2=['Lets understand is of words is']\n",
    "\n",
    "# another instance\n",
    "c_new_vect=c_vect.transform(string2)\n",
    "print(\"text Present at \",c_new_vect.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "19665ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Indexes ['bag' 'example' 'is' 'of' 'this' 'words']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Indexes\",vect1.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6e53e069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Vocab:  {'this': 4, 'is': 2, 'example': 1, 'of': 3, 'bag': 0, 'words': 5}\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Vocab: \",vect1.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efed652d",
   "metadata": {},
   "source": [
    "**0 shows nill bag is present, IS is present hence shows 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "baad5aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
      "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
      "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
      "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
      "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
      "                            'itself', ...])\n"
     ]
    }
   ],
   "source": [
    "## Bag of words using stopwords (you can avoid writing extra steps to remove stopwords)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stp=stopwords.words(\"english\")\n",
    "\n",
    "string=[\"This is an example of bag of words!\"]\n",
    "vect1=CountVectorizer(stop_words=stp)\n",
    "print(vect1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c11e2e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is an example of bag of words!']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check string\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0939663b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW  ['bag' 'example' 'words']\n",
      "Vocab {'example': 1, 'bag': 0, 'words': 2}\n"
     ]
    }
   ],
   "source": [
    "vect1.fit_transform(string)\n",
    "\n",
    "# show the names\n",
    "print(\"BOW \",vect1.get_feature_names_out())\n",
    "\n",
    "# will show the index and values\n",
    "print(\"Vocab\",vect1.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca77e9c",
   "metadata": {},
   "source": [
    "# Using Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e15c0484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# built function\n",
    "def text_matrix(message,countvect):\n",
    "    terms_doc=countvect.fit_transform(message)\n",
    "    return pd.DataFrame(terms_doc.toarray(),columns=countvect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "81f003cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act</th>\n",
       "      <th>alphabetic</th>\n",
       "      <th>characters</th>\n",
       "      <th>composing</th>\n",
       "      <th>consisting</th>\n",
       "      <th>devices</th>\n",
       "      <th>electronic</th>\n",
       "      <th>messages</th>\n",
       "      <th>messaging</th>\n",
       "      <th>mobile</th>\n",
       "      <th>numeric</th>\n",
       "      <th>sending</th>\n",
       "      <th>text</th>\n",
       "      <th>texting</th>\n",
       "      <th>two</th>\n",
       "      <th>typically</th>\n",
       "      <th>users</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   act  alphabetic  characters  composing  consisting  devices  electronic  \\\n",
       "0    1           0           0          1           0        0           1   \n",
       "1    0           1           1          0           1        0           0   \n",
       "2    0           0           0          0           0        1           0   \n",
       "\n",
       "   messages  messaging  mobile  numeric  sending  text  texting  two  \\\n",
       "0         1          1       0        0        1     1        1    0   \n",
       "1         0          0       0        1        0     0        0    0   \n",
       "2         0          0       1        0        0     0        0    1   \n",
       "\n",
       "   typically  users  \n",
       "0          0      0  \n",
       "1          1      0  \n",
       "2          0      1  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message=['Text messaging, or texting, is the act of composing and sending electronic messages', \n",
    "         'typically consisting of alphabetic and numeric characters', \n",
    "         'between two or more users of mobile devices']\n",
    "\n",
    "c_vect=CountVectorizer(stop_words=stp)\n",
    "\n",
    "text_matrix(message,c_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da924e4a",
   "metadata": {},
   "source": [
    "# N Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeda085b",
   "metadata": {},
   "source": [
    "N-grams is present in **countvectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7fb752e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni Gram:  ['am' 'bigram' 'learning' 'trigram' 'unigram'] \n",
      " {'am': 0, 'learning': 2, 'unigram': 4, 'bigram': 1, 'trigram': 3}\n",
      "\n",
      "Bi Gram:  ['am learning' 'bigram trigram' 'learning unigram' 'unigram bigram'] \n",
      " {'am learning': 0, 'learning unigram': 2, 'unigram bigram': 3, 'bigram trigram': 1}\n",
      "\n",
      "Tri Gram:  ['am learning unigram' 'learning unigram bigram' 'unigram bigram trigram'] \n",
      " {'am learning unigram': 0, 'learning unigram bigram': 1, 'unigram bigram trigram': 2}\n",
      "\n",
      "Four Gram: ['am learning unigram bigram' 'learning unigram bigram trigram'] \n",
      " {'am learning unigram bigram': 0, 'learning unigram bigram trigram': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "string=[\"I am Learning UniGram, BiGram, TriGram\"]\n",
    "\n",
    "vect1=CountVectorizer(ngram_range=(1,1))\n",
    "vect1.fit_transform(string)\n",
    "\n",
    "vect2=CountVectorizer(ngram_range=(2,2))\n",
    "vect2.fit_transform(string)\n",
    "\n",
    "vect3=CountVectorizer(ngram_range=(3,3))\n",
    "vect3.fit_transform(string)\n",
    "\n",
    "vect4=CountVectorizer(ngram_range=(4,4))\n",
    "vect4.fit_transform(string)\n",
    "\n",
    "print(\"Uni Gram: \",vect1.get_feature_names_out(),\"\\n\", vect1.vocabulary_)\n",
    "print(\"\\nBi Gram: \",vect2.get_feature_names_out(),\"\\n\", vect2.vocabulary_)\n",
    "print(\"\\nTri Gram: \",vect3.get_feature_names_out(),\"\\n\",vect3.vocabulary_)\n",
    "print(\"\\nFour Gram:\",vect4.get_feature_names_out(),\"\\n\",vect4.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "923fbee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffffab4",
   "metadata": {},
   "source": [
    "# TF/IDF(Term Frequency-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11f9c8c",
   "metadata": {},
   "source": [
    "**Term Frequency**=(Number of times a word apprears in the document)**/**(Total number of words in the document)\n",
    "\n",
    "Example: Hello I am learning how to say Hello\n",
    "\n",
    "        TF(HELLO)= 2/8 TF(I)=1/8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4494c21e",
   "metadata": {},
   "source": [
    "**Inverse Document Frequency**\n",
    "--------------------------------------------\n",
    "\n",
    "Inverse Document Freq\n",
    "\n",
    "log[(Number of documents)/(Number of documents the word appears in)]\n",
    "\n",
    "\n",
    "- NOTE: log has base 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5452712c",
   "metadata": {},
   "source": [
    "**TF-IDF= Term Frequency*Inverse Document Frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3236f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>be</th>\n",
       "      <th>can</th>\n",
       "      <th>confusing</th>\n",
       "      <th>exam</th>\n",
       "      <th>how</th>\n",
       "      <th>idf</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>see</th>\n",
       "      <th>this</th>\n",
       "      <th>we</th>\n",
       "      <th>will</th>\n",
       "      <th>works</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    an   be  can  confusing  exam       how  idf   is        it       see  this        we      will     works\n",
       "0  0.5  0.0  0.0        0.0   0.5  0.000000  0.0  0.5  0.000000  0.000000   0.5  0.000000  0.000000  0.000000\n",
       "1  0.0  0.0  0.0        0.0   0.0  0.408248  0.0  0.0  0.408248  0.408248   0.0  0.408248  0.408248  0.408248\n",
       "2  0.0  0.5  0.5        0.5   0.0  0.000000  0.5  0.0  0.000000  0.000000   0.0  0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "tfid=TfidfVectorizer(smooth_idf=False)\n",
    "\n",
    "doc=[\"This is an Exam.\",\"We will see how it works.\",\"IDF can be confusing\"]\n",
    "doc_vector=tfid.fit_transform(doc)\n",
    "\n",
    "df=pd.DataFrame(doc_vector.todense(),columns=tfid.get_feature_names_out())\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee967107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>cases</th>\n",
       "      <th>covid</th>\n",
       "      <th>dropping</th>\n",
       "      <th>is</th>\n",
       "      <th>nothing</th>\n",
       "      <th>that</th>\n",
       "      <th>what</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.592567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501651</td>\n",
       "      <td>0.501651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.322745</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        are     cases     covid  dropping        is   nothing      that      what\n",
       "0  0.000000  0.000000  0.592567  0.000000  0.381519  0.000000  0.501651  0.501651\n",
       "1  0.000000  0.000000  0.425441  0.000000  0.547832  0.720333  0.000000  0.000000\n",
       "2  0.546454  0.546454  0.322745  0.546454  0.000000  0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using function\n",
    "\n",
    "def text_matrix(message,model):\n",
    "    terms_doc=model.fit_transform(message)\n",
    "    return pd.DataFrame(terms_doc.toarray(),columns=model.get_feature_names_out())\n",
    "\n",
    "# we will call the function created earlier\n",
    "\n",
    "document=[\"What is that covid covid\",\"covid is nothing\",\"Covid cases are dropping\"]\n",
    "\n",
    "tf=TfidfVectorizer()\n",
    "\n",
    "text_matrix(document,tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "114a8c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[\"Car was cleaned by Jack\",\n",
    "     \"Jack was cleaned by car\"]\n",
    "\n",
    "# import count vectorizer,TfidVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1183166d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Name\n",
      " ['by car' 'by jack' 'car was' 'cleaned by' 'jack was' 'was cleaned']\n",
      "Array\n",
      "--------------\n",
      " [[0 1 1 1 0 1]\n",
      " [1 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# If you want to take into account just term frequencies\n",
    "vectorizer=CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "#ngram range specifies your ngram confrigation.\n",
    "x=vectorizer.fit_transform(text)\n",
    "\n",
    "#testing the ngram genration:\n",
    "print(\"Feature Name\\n\",vectorizer.get_feature_names_out())\n",
    "print(\"Array\\n--------------\\n\",x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a3bea46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['by car' 'by jack' 'car was' 'cleaned by' 'jack was' 'was cleaned']\n",
      "[[0.         0.57615236 0.57615236 0.40993715 0.         0.40993715]\n",
      " [0.57615236 0.         0.         0.40993715 0.57615236 0.40993715]]\n"
     ]
    }
   ],
   "source": [
    "# And now testing TFIDF vectorizer:\n",
    "# you can still specify n-gram here.\n",
    "vectorizer=TfidfVectorizer(ngram_range=(2,2))\n",
    "x=vectorizer.fit_transform(text)\n",
    "\n",
    "#Testing\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176f5ff",
   "metadata": {},
   "source": [
    "**Without Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34e32fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.40546511 1.40546511 1.         0.         1.        ]\n",
      " [1.40546511 0.         0.         1.         1.40546511 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Testing TFIDF vectorizer without normalization:\n",
    "# you can still specify n-gram here.\n",
    "\n",
    "vectorizer=TfidfVectorizer(ngram_range=(2,2),norm=None)\n",
    "x=vectorizer.fit_transform(text)\n",
    "\n",
    "#testing\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555c137",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "779b9369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precise\n",
    "# more efficient and advanced technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d516df17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "# import word2vec\n",
    "from gensim.models import Word2Vec # genrate all many vectors\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.stem import LancasterStemmer,PorterStemmer,SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from string import punctuation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42d60f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"Coronaviruses are a group of related RNA viruses that cause diseases in mammals and birds. In humans and birds, they cause respiratory tract infections that can range from mild to lethal. Mild illnesses in humans include some cases of the common cold (which is also caused by other viruses, predominantly rhinoviruses), while more lethal varieties can cause SARS, MERS and COVID-19, which is causing the ongoing pandemic. In cows and pigs they cause diarrhea, while in mice they cause hepatitis and encephalomyelitis.\n",
    "\n",
    "Coronaviruses constitute the subfamily Orthocoronavirinae, in the family Coronaviridae, order Nidovirales and realm Riboviria.[3][4] They are enveloped viruses with a positive-sense single-stranded RNA genome and a nucleocapsid of helical symmetry.[5] The genome size of coronaviruses ranges from approximately 26 to 32 kilobases, one of the largest among RNA viruses.[6] They have characteristic club-shaped spikes that project from their surface, which in electron micrographs create an image reminiscent of the stellar corona, from which their name derives.[7].The name \"coronavirus\" is derived from Latin corona, meaning \"crown\" or \"wreath\", itself a borrowing from Greek κορώνη korṓnē, \"garland, wreath\".[8][9] The name was coined by June Almeida and David Tyrrell who first observed and studied human coronaviruses.[10] The word was first used in print in 1968 by an informal group of virologists in the journal Nature to designate the new family of viruses.[7] The name refers to the characteristic appearance of virions (the infective form of the virus) by electron microscopy, which have a fringe of large, bulbous surface projections creating an image reminiscent of the solar corona or halo.[7][10] This morphology is created by the viral spike peplomers, which are proteins on the surface of the virus.[11]\n",
    "\n",
    "The scientific name Coronavirus was accepted as a genus name by the International Committee for the Nomenclature of Viruses (later renamed International Committee on Taxonomy of Viruses) in 1971.[12] As the number of new species increased, the genus was split into four genera, namely Alphacoronavirus, Betacoronavirus, Deltacoronavirus, and Gammacoronavirus in 2009.[13] The common name coronavirus is used to refer to any member of the subfamily Orthocoronavirinae.[4] As of 2020, 45 species are officially recognised.[14]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c30b6b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coronaviruses are a group of related RNA viruses that cause diseases in mammals and birds. In humans and birds, they cause respiratory tract infections that can range from mild to lethal. Mild illnesses in humans include some cases of the common cold (which is also caused by other viruses, predominantly rhinoviruses), while more lethal varieties can cause SARS, MERS and COVID-19, which is causing the ongoing pandemic. In cows and pigs they cause diarrhea, while in mice they cause hepatitis and encephalomyelitis.\\n\\nCoronaviruses constitute the subfamily Orthocoronavirinae, in the family Coronaviridae, order Nidovirales and realm Riboviria.[3][4] They are enveloped viruses with a positive-sense single-stranded RNA genome and a nucleocapsid of helical symmetry.[5] The genome size of coronaviruses ranges from approximately 26 to 32 kilobases, one of the largest among RNA viruses.[6] They have characteristic club-shaped spikes that project from their surface, which in electron micrographs create an image reminiscent of the stellar corona, from which their name derives.[7].The name \"coronavirus\" is derived from Latin corona, meaning \"crown\" or \"wreath\", itself a borrowing from Greek κορώνη korṓnē, \"garland, wreath\".[8][9] The name was coined by June Almeida and David Tyrrell who first observed and studied human coronaviruses.[10] The word was first used in print in 1968 by an informal group of virologists in the journal Nature to designate the new family of viruses.[7] The name refers to the characteristic appearance of virions (the infective form of the virus) by electron microscopy, which have a fringe of large, bulbous surface projections creating an image reminiscent of the solar corona or halo.[7][10] This morphology is created by the viral spike peplomers, which are proteins on the surface of the virus.[11]\\n\\nThe scientific name Coronavirus was accepted as a genus name by the International Committee for the Nomenclature of Viruses (later renamed International Committee on Taxonomy of Viruses) in 1971.[12] As the number of new species increased, the genus was split into four genera, namely Alphacoronavirus, Betacoronavirus, Deltacoronavirus, and Gammacoronavirus in 2009.[13] The common name coronavirus is used to refer to any member of the subfamily Orthocoronavirinae.[4] As of 2020, 45 species are officially recognised.[14]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b07e12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coronaviruses are a group of related rna viruses that cause diseases in mammals and birds in humans and birds they cause respiratory tract infections that can range from mild to lethal mild illnesses in humans include some cases of the common cold which is also caused by other viruses predominantly rhinoviruses while more lethal varieties can cause sars mers and covid which is causing the ongoing pandemic in cows and pigs they cause diarrhea while in mice they cause hepatitis and encephalomyelitis coronaviruses constitute the subfamily orthocoronavirinae in the family coronaviridae order nidovirales and realm riboviria they are enveloped viruses with a positivesense singlestranded rna genome and a nucleocapsid of helical symmetry the genome size of coronaviruses ranges from approximately to kilobases one of the largest among rna viruses they have characteristic clubshaped spikes that project from their surface which in electron micrographs create an image reminiscent of the stellar corona from which their name derives the name coronavirus is derived from latin corona meaning crown or wreath itself a borrowing from greek κορώνη korṓnē garland wreath the name was coined by june almeida and david tyrrell who first observed and studied human coronaviruses the word was first used in print in by an informal group of virologists in the journal nature to designate the new family of viruses the name refers to the characteristic appearance of virions the infective form of the virus by electron microscopy which have a fringe of large bulbous surface projections creating an image reminiscent of the solar corona or halo this morphology is created by the viral spike peplomers which are proteins on the surface of the virus the scientific name coronavirus was accepted as a genus name by the international committee for the nomenclature of viruses later renamed international committee on taxonomy of viruses in as the number of new species increased the genus was split into four genera namely alphacoronavirus betacoronavirus deltacoronavirus and gammacoronavirus in the common name coronavirus is used to refer to any member of the subfamily orthocoronavirinae as of species are officially recognised \n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the data (from more details on regular expression visit :https://regexr.com//)\n",
    "\n",
    "para=paragraph.translate(str.maketrans(\" \",\" \",string.punctuation)) # replace punctuation with spaces\n",
    "\n",
    "#numbers remove\n",
    "text=re.sub(r'\\[[0-9]*]',' ',para)\n",
    "# special char remove\n",
    "text=re.sub(r'\\s+',' ',text)\n",
    "# lower\n",
    "text=text.lower()\n",
    "\n",
    "text=re.sub(r'\\d',' ',text)\n",
    "text=re.sub(r'\\s+',' ',text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1382830b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coronaviruses are a group of related rna viruses that cause diseases in mammals and birds in humans and birds they cause respiratory tract infections that can range from mild to lethal mild illnesses in humans include some cases of the common cold which is also caused by other viruses predominantly rhinoviruses while more lethal varieties can cause sars mers and covid which is causing the ongoing pandemic in cows and pigs they cause diarrhea while in mice they cause hepatitis and encephalomyelitis coronaviruses constitute the subfamily orthocoronavirinae in the family coronaviridae order nidovirales and realm riboviria they are enveloped viruses with a positivesense singlestranded rna genome and a nucleocapsid of helical symmetry the genome size of coronaviruses ranges from approximately to kilobases one of the largest among rna viruses they have characteristic clubshaped spikes that project from their surface which in electron micrographs create an image reminiscent of the stellar corona from which their name derives the name coronavirus is derived from latin corona meaning crown or wreath itself a borrowing from greek κορώνη korṓnē garland wreath the name was coined by june almeida and david tyrrell who first observed and studied human coronaviruses the word was first used in print in by an informal group of virologists in the journal nature to designate the new family of viruses the name refers to the characteristic appearance of virions the infective form of the virus by electron microscopy which have a fringe of large bulbous surface projections creating an image reminiscent of the solar corona or halo this morphology is created by the viral spike peplomers which are proteins on the surface of the virus the scientific name coronavirus was accepted as a genus name by the international committee for the nomenclature of viruses later renamed international committee on taxonomy of viruses in as the number of new species increased the genus was split into four genera namely alphacoronavirus betacoronavirus deltacoronavirus and gammacoronavirus in the common name coronavirus is used to refer to any member of the subfamily orthocoronavirinae as of species are officially recognised']\n"
     ]
    }
   ],
   "source": [
    "# Preparing the dataset\n",
    "sen=nltk.sent_tokenize(text)\n",
    "print(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1c5cb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['coronaviruses', 'are', 'a', 'group', 'of', 'related', 'rna', 'viruses', 'that', 'cause', 'diseases', 'in', 'mammals', 'and', 'birds', 'in', 'humans', 'and', 'birds', 'they', 'cause', 'respiratory', 'tract', 'infections', 'that', 'can', 'range', 'from', 'mild', 'to', 'lethal', 'mild', 'illnesses', 'in', 'humans', 'include', 'some', 'cases', 'of', 'the', 'common', 'cold', 'which', 'is', 'also', 'caused', 'by', 'other', 'viruses', 'predominantly', 'rhinoviruses', 'while', 'more', 'lethal', 'varieties', 'can', 'cause', 'sars', 'mers', 'and', 'covid', 'which', 'is', 'causing', 'the', 'ongoing', 'pandemic', 'in', 'cows', 'and', 'pigs', 'they', 'cause', 'diarrhea', 'while', 'in', 'mice', 'they', 'cause', 'hepatitis', 'and', 'encephalomyelitis', 'coronaviruses', 'constitute', 'the', 'subfamily', 'orthocoronavirinae', 'in', 'the', 'family', 'coronaviridae', 'order', 'nidovirales', 'and', 'realm', 'riboviria', 'they', 'are', 'enveloped', 'viruses', 'with', 'a', 'positivesense', 'singlestranded', 'rna', 'genome', 'and', 'a', 'nucleocapsid', 'of', 'helical', 'symmetry', 'the', 'genome', 'size', 'of', 'coronaviruses', 'ranges', 'from', 'approximately', 'to', 'kilobases', 'one', 'of', 'the', 'largest', 'among', 'rna', 'viruses', 'they', 'have', 'characteristic', 'clubshaped', 'spikes', 'that', 'project', 'from', 'their', 'surface', 'which', 'in', 'electron', 'micrographs', 'create', 'an', 'image', 'reminiscent', 'of', 'the', 'stellar', 'corona', 'from', 'which', 'their', 'name', 'derives', 'the', 'name', 'coronavirus', 'is', 'derived', 'from', 'latin', 'corona', 'meaning', 'crown', 'or', 'wreath', 'itself', 'a', 'borrowing', 'from', 'greek', 'κορώνη', 'korṓnē', 'garland', 'wreath', 'the', 'name', 'was', 'coined', 'by', 'june', 'almeida', 'and', 'david', 'tyrrell', 'who', 'first', 'observed', 'and', 'studied', 'human', 'coronaviruses', 'the', 'word', 'was', 'first', 'used', 'in', 'print', 'in', 'by', 'an', 'informal', 'group', 'of', 'virologists', 'in', 'the', 'journal', 'nature', 'to', 'designate', 'the', 'new', 'family', 'of', 'viruses', 'the', 'name', 'refers', 'to', 'the', 'characteristic', 'appearance', 'of', 'virions', 'the', 'infective', 'form', 'of', 'the', 'virus', 'by', 'electron', 'microscopy', 'which', 'have', 'a', 'fringe', 'of', 'large', 'bulbous', 'surface', 'projections', 'creating', 'an', 'image', 'reminiscent', 'of', 'the', 'solar', 'corona', 'or', 'halo', 'this', 'morphology', 'is', 'created', 'by', 'the', 'viral', 'spike', 'peplomers', 'which', 'are', 'proteins', 'on', 'the', 'surface', 'of', 'the', 'virus', 'the', 'scientific', 'name', 'coronavirus', 'was', 'accepted', 'as', 'a', 'genus', 'name', 'by', 'the', 'international', 'committee', 'for', 'the', 'nomenclature', 'of', 'viruses', 'later', 'renamed', 'international', 'committee', 'on', 'taxonomy', 'of', 'viruses', 'in', 'as', 'the', 'number', 'of', 'new', 'species', 'increased', 'the', 'genus', 'was', 'split', 'into', 'four', 'genera', 'namely', 'alphacoronavirus', 'betacoronavirus', 'deltacoronavirus', 'and', 'gammacoronavirus', 'in', 'the', 'common', 'name', 'coronavirus', 'is', 'used', 'to', 'refer', 'to', 'any', 'member', 'of', 'the', 'subfamily', 'orthocoronavirinae', 'as', 'of', 'species', 'are', 'officially', 'recognised']]\n"
     ]
    }
   ],
   "source": [
    "# prepare word tokenization\n",
    "word=[nltk.word_tokenize(sen) for sen in sen]\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69857080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc=string.punctuation\n",
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "012a1c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(word)):\n",
    "    word[i]=[word for word in word[i]\n",
    "                  if word not in stopwords.words('english') if word not in punc]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ef10199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['coronaviruses',\n",
       "  'group',\n",
       "  'related',\n",
       "  'rna',\n",
       "  'viruses',\n",
       "  'cause',\n",
       "  'diseases',\n",
       "  'mammals',\n",
       "  'birds',\n",
       "  'humans',\n",
       "  'birds',\n",
       "  'cause',\n",
       "  'respiratory',\n",
       "  'tract',\n",
       "  'infections',\n",
       "  'range',\n",
       "  'mild',\n",
       "  'lethal',\n",
       "  'mild',\n",
       "  'illnesses',\n",
       "  'humans',\n",
       "  'include',\n",
       "  'cases',\n",
       "  'common',\n",
       "  'cold',\n",
       "  'also',\n",
       "  'caused',\n",
       "  'viruses',\n",
       "  'predominantly',\n",
       "  'rhinoviruses',\n",
       "  'lethal',\n",
       "  'varieties',\n",
       "  'cause',\n",
       "  'sars',\n",
       "  'mers',\n",
       "  'covid',\n",
       "  'causing',\n",
       "  'ongoing',\n",
       "  'pandemic',\n",
       "  'cows',\n",
       "  'pigs',\n",
       "  'cause',\n",
       "  'diarrhea',\n",
       "  'mice',\n",
       "  'cause',\n",
       "  'hepatitis',\n",
       "  'encephalomyelitis',\n",
       "  'coronaviruses',\n",
       "  'constitute',\n",
       "  'subfamily',\n",
       "  'orthocoronavirinae',\n",
       "  'family',\n",
       "  'coronaviridae',\n",
       "  'order',\n",
       "  'nidovirales',\n",
       "  'realm',\n",
       "  'riboviria',\n",
       "  'enveloped',\n",
       "  'viruses',\n",
       "  'positivesense',\n",
       "  'singlestranded',\n",
       "  'rna',\n",
       "  'genome',\n",
       "  'nucleocapsid',\n",
       "  'helical',\n",
       "  'symmetry',\n",
       "  'genome',\n",
       "  'size',\n",
       "  'coronaviruses',\n",
       "  'ranges',\n",
       "  'approximately',\n",
       "  'kilobases',\n",
       "  'one',\n",
       "  'largest',\n",
       "  'among',\n",
       "  'rna',\n",
       "  'viruses',\n",
       "  'characteristic',\n",
       "  'clubshaped',\n",
       "  'spikes',\n",
       "  'project',\n",
       "  'surface',\n",
       "  'electron',\n",
       "  'micrographs',\n",
       "  'create',\n",
       "  'image',\n",
       "  'reminiscent',\n",
       "  'stellar',\n",
       "  'corona',\n",
       "  'name',\n",
       "  'derives',\n",
       "  'name',\n",
       "  'coronavirus',\n",
       "  'derived',\n",
       "  'latin',\n",
       "  'corona',\n",
       "  'meaning',\n",
       "  'crown',\n",
       "  'wreath',\n",
       "  'borrowing',\n",
       "  'greek',\n",
       "  'κορώνη',\n",
       "  'korṓnē',\n",
       "  'garland',\n",
       "  'wreath',\n",
       "  'name',\n",
       "  'coined',\n",
       "  'june',\n",
       "  'almeida',\n",
       "  'david',\n",
       "  'tyrrell',\n",
       "  'first',\n",
       "  'observed',\n",
       "  'studied',\n",
       "  'human',\n",
       "  'coronaviruses',\n",
       "  'word',\n",
       "  'first',\n",
       "  'used',\n",
       "  'print',\n",
       "  'informal',\n",
       "  'group',\n",
       "  'virologists',\n",
       "  'journal',\n",
       "  'nature',\n",
       "  'designate',\n",
       "  'new',\n",
       "  'family',\n",
       "  'viruses',\n",
       "  'name',\n",
       "  'refers',\n",
       "  'characteristic',\n",
       "  'appearance',\n",
       "  'virions',\n",
       "  'infective',\n",
       "  'form',\n",
       "  'virus',\n",
       "  'electron',\n",
       "  'microscopy',\n",
       "  'fringe',\n",
       "  'large',\n",
       "  'bulbous',\n",
       "  'surface',\n",
       "  'projections',\n",
       "  'creating',\n",
       "  'image',\n",
       "  'reminiscent',\n",
       "  'solar',\n",
       "  'corona',\n",
       "  'halo',\n",
       "  'morphology',\n",
       "  'created',\n",
       "  'viral',\n",
       "  'spike',\n",
       "  'peplomers',\n",
       "  'proteins',\n",
       "  'surface',\n",
       "  'virus',\n",
       "  'scientific',\n",
       "  'name',\n",
       "  'coronavirus',\n",
       "  'accepted',\n",
       "  'genus',\n",
       "  'name',\n",
       "  'international',\n",
       "  'committee',\n",
       "  'nomenclature',\n",
       "  'viruses',\n",
       "  'later',\n",
       "  'renamed',\n",
       "  'international',\n",
       "  'committee',\n",
       "  'taxonomy',\n",
       "  'viruses',\n",
       "  'number',\n",
       "  'new',\n",
       "  'species',\n",
       "  'increased',\n",
       "  'genus',\n",
       "  'split',\n",
       "  'four',\n",
       "  'genera',\n",
       "  'namely',\n",
       "  'alphacoronavirus',\n",
       "  'betacoronavirus',\n",
       "  'deltacoronavirus',\n",
       "  'gammacoronavirus',\n",
       "  'common',\n",
       "  'name',\n",
       "  'coronavirus',\n",
       "  'used',\n",
       "  'refer',\n",
       "  'member',\n",
       "  'subfamily',\n",
       "  'orthocoronavirinae',\n",
       "  'species',\n",
       "  'officially',\n",
       "  'recognised']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e86c7593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the word2vec model\n",
    "\n",
    "model=Word2Vec(word,min_count=1)\n",
    "\n",
    "words=model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9d70ae9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.9931917e-04  1.4241856e-03 -1.1763177e-03 -4.0283194e-03\n",
      "  8.8075799e-04 -9.6924528e-03  1.4212002e-03 -4.1426546e-03\n",
      " -5.8331871e-03  3.8086812e-03  1.4410870e-03  3.6084182e-03\n",
      "  8.9708465e-04 -9.5073180e-03  1.0007575e-02 -3.5515660e-03\n",
      " -2.2044803e-03  7.1295216e-03  6.9474590e-05  3.0221124e-03\n",
      "  2.2675982e-03  2.3988588e-03  6.7407237e-03  9.4494168e-03\n",
      " -5.6322868e-04 -3.4326576e-03  4.5475299e-03 -8.3288644e-03\n",
      "  8.4672906e-03  2.3880857e-03  6.0262373e-03  2.7934159e-03\n",
      " -1.3372272e-03  9.3967159e-04 -4.8147328e-03  9.1099227e-03\n",
      "  5.0677392e-03  3.6057378e-03 -5.0045708e-03 -8.6835148e-03\n",
      "  7.9308283e-03  2.9333455e-03 -4.2431713e-03 -8.6511290e-03\n",
      " -3.9551514e-03  1.3998369e-03 -9.2200777e-03  6.9566842e-05\n",
      "  2.8164573e-03 -9.9367369e-03 -1.4605212e-04 -6.0979901e-03\n",
      " -3.6046780e-03 -1.6483793e-03 -9.9887336e-03 -3.5967347e-03\n",
      " -7.0089353e-03 -7.3295729e-03  1.5863036e-03  2.3445836e-03\n",
      "  9.3028154e-03  8.8724727e-03 -6.3858186e-03 -9.2414431e-03\n",
      "  7.8761410e-03  5.4432619e-03  6.9436789e-03 -2.9019574e-03\n",
      " -5.2695205e-03 -4.5609176e-03 -8.3496459e-03  5.8988114e-03\n",
      " -3.4291956e-03  8.6309863e-03 -5.6451256e-03  9.6521247e-03\n",
      " -4.5076115e-03  8.6738588e-03 -5.4375054e-03 -7.5117708e-03\n",
      "  7.6568844e-03 -9.7661503e-03 -8.8320076e-03  4.9788235e-03\n",
      " -9.7590908e-03  5.2879290e-03  3.6620377e-03 -7.0180804e-03\n",
      "  3.1944022e-03  1.4455235e-03  8.6599188e-03  2.9263098e-03\n",
      " -5.1113917e-03 -2.3614017e-03  6.8962751e-03 -6.6089681e-03\n",
      "  9.3029905e-03 -3.2176930e-03 -4.6191253e-03 -4.3960693e-03]\n"
     ]
    }
   ],
   "source": [
    "# Test the word vector\n",
    "vector=model.wv['covid']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c6c241d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.1470190e-03, -4.4672932e-03, -1.0516491e-03,  1.0043441e-03,\n",
       "       -1.8348334e-04,  1.1185758e-03,  6.1323992e-03,  1.9062081e-05,\n",
       "       -3.2801626e-03, -1.5398301e-03,  5.8873366e-03,  1.4732070e-03,\n",
       "       -7.2866323e-04,  9.3518263e-03, -4.8961439e-03, -8.4017246e-04,\n",
       "        9.1434233e-03,  6.7317914e-03,  1.4990760e-03, -8.9043835e-03,\n",
       "        1.1575153e-03, -2.2758816e-03,  9.3526710e-03,  1.1786288e-03,\n",
       "        1.4806595e-03,  2.4057324e-03, -1.8435704e-03, -5.0093126e-03,\n",
       "        2.3134725e-04, -2.0162675e-03,  6.6284756e-03,  8.9214155e-03,\n",
       "       -6.7644607e-04,  2.9656598e-03, -6.1128079e-03,  1.7368305e-03,\n",
       "       -6.8978006e-03, -8.6955233e-03, -5.8968016e-03, -8.9870812e-03,\n",
       "        7.2776480e-03, -5.7785194e-03,  8.2704742e-03, -7.2335382e-03,\n",
       "        3.4319602e-03,  9.6661896e-03, -7.8161350e-03, -9.9403346e-03,\n",
       "       -4.3126522e-03, -2.6746027e-03, -2.6755061e-04, -8.8506658e-03,\n",
       "       -8.6038671e-03,  2.7740002e-03, -8.2076602e-03, -9.0394905e-03,\n",
       "       -2.3300659e-03, -8.6126486e-03, -7.0669265e-03, -8.3991336e-03,\n",
       "       -3.0437196e-04, -4.5501920e-03,  6.6358210e-03,  1.5125780e-03,\n",
       "       -3.3613185e-03,  6.1274273e-03, -6.0006990e-03, -4.6262387e-03,\n",
       "       -7.2264578e-03, -4.3258700e-03, -1.8353889e-03,  6.5084198e-03,\n",
       "       -2.7664979e-03,  4.9165166e-03,  6.9440450e-03, -7.4457503e-03,\n",
       "        4.5537986e-03,  6.1395904e-03, -2.9768820e-03,  6.6275275e-03,\n",
       "        6.1097704e-03, -6.4279400e-03, -6.8004052e-03,  2.5875317e-03,\n",
       "       -1.6466865e-03, -6.0538687e-03,  9.4975587e-03, -5.1143039e-03,\n",
       "       -6.5124170e-03, -1.3129442e-04, -2.6826395e-03,  4.8178618e-04,\n",
       "       -3.5400430e-03, -4.1523259e-04, -6.7841070e-04,  8.4280438e-04,\n",
       "        8.2004275e-03, -5.7562687e-03, -1.6353171e-03,  5.5509158e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector2=model.wv['coronavirus']\n",
    "vector2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d509a589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('among', 0.3491682708263397),\n",
       " ('reminiscent', 0.3037218749523163),\n",
       " ('helical', 0.24839727580547333),\n",
       " ('covid', 0.2228318303823471),\n",
       " ('image', 0.17787884175777435)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most similar word\n",
    "\n",
    "similar=model.wv.most_similar(\"coronavirus\",topn=5)\n",
    "similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a1a40aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3491683"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word semilarity\n",
    "model.wv.similarity(w1='coronavirus',w2='among')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8fc7f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corona'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter not similar\n",
    "model.wv.doesnt_match([\"corona\",\"coronavirus\",\"among\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421786da",
   "metadata": {},
   "source": [
    "# Cosign Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037b0b93",
   "metadata": {},
   "source": [
    "from sklearn.manifold import TSNE # TSNE is same like PCA\n",
    "vocab=['infection','coronavirus','spread','human']\n",
    "\n",
    "def tsne_plot(model):\n",
    "    labels=[]\n",
    "    wordvecs=[]\n",
    "    \n",
    "    for word in vocab:\n",
    "        wordvecs.append(model[word])\n",
    "        labels.append(word)\n",
    "        \n",
    "    tsne_model=TSNE(perplexity=3,init='pca',random_state=42)\n",
    "    coordinates=tsne_model.fit_transform(wordvecs)\n",
    "    \n",
    "    x=[]\n",
    "    y=[]\n",
    "    \n",
    "    for value in coordinates:\n",
    "        x.append(value[[0]])\n",
    "        y.append(value[[1]])\n",
    "        \n",
    "    plt.figure(figsize=(10,6))\n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i],y[i]),\n",
    "                     xytext=(2,2),\n",
    "                     textcoods='offset points',ha='right')\n",
    "    plt.show()\n",
    "    \n",
    "tsne_plot(model)\n",
    "\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cc39ec",
   "metadata": {},
   "source": [
    "# GENSIM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59bb0e8",
   "metadata": {},
   "source": [
    "### What is Gensim?\n",
    "- Gensim= \"Generate Similar\" is a popular **open source natural language Processing(NLP) library used for supervised topic modeling**.\n",
    "\n",
    "1. Document: some text\n",
    "2. Corpus: a collection of documents\n",
    "3. Vector: Mathematically convenient representation of a document.\n",
    "4. Model: An algorithm for transforming vectors from one respresentation to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "269329aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOME TEXT\n",
    "\n",
    "Document=\"Human Machine interface for lab abc computer applications\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b6edaf",
   "metadata": {},
   "source": [
    "**CORPUS** :Collections of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d658dd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus=[\"Human machine interface for lab abc computer application\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",\n",
    "            \"Realtion of user perceived response time to error measurement\",\n",
    "            \"Then genration of random binary un-ordered trees\",\n",
    "            \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "            \"Graph minors A Survey\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e56f14e",
   "metadata": {},
   "source": [
    "***VECTOR: A mathematically convenient representation of a document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "545ff369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use pprint for gensim \n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ac7c18",
   "metadata": {},
   "source": [
    "**insall gensim** !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "126318c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'for', 'of', 'and', 'the', 'in', 'a', 'to'}\n"
     ]
    }
   ],
   "source": [
    "# create set of frequent words\n",
    "stoplist=set(\"for a of the and to in\".split(\" \"))\n",
    "print(stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd8d29b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# lowercase each document, split it by white space and filter out stopwords\n",
    "\n",
    "texts=[[word for word in document.lower().split() if word not in stoplist] for document in text_corpus] \n",
    "# steps: full the sentance-> check for stopwords in stoplist if no store in texts\n",
    "\n",
    "# Count word frequencies:\n",
    "from collections import defaultdict\n",
    "\n",
    "frequency=defaultdict(int)\n",
    "\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token]+=1\n",
    "        \n",
    "# only keep words that appears more than once\n",
    "processed_corpus=[[token for token in text if frequency[token]>1]  for text in texts]\n",
    "\n",
    "\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b243ca6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...>\n"
     ]
    }
   ],
   "source": [
    "# creating dictionary, which helps during Topic Modeling\n",
    "from gensim import corpora\n",
    "\n",
    "dictionary= corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e888dbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0,\n",
      " 'eps': 8,\n",
      " 'graph': 10,\n",
      " 'human': 1,\n",
      " 'interface': 2,\n",
      " 'minors': 11,\n",
      " 'response': 3,\n",
      " 'survey': 4,\n",
      " 'system': 5,\n",
      " 'time': 6,\n",
      " 'trees': 9,\n",
      " 'user': 7}\n"
     ]
    }
   ],
   "source": [
    "# Vectore Representation\n",
    "pprint.pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e928505f",
   "metadata": {},
   "source": [
    "#### MODEL: An algorithm for transforming vectors from one representation to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6c7d42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "# always make a practice of testing small texts when trying something new\n",
    "# in each tuple below 1st occurance is ID and 2nd Ocurrance is count\n",
    "new_doc=\"Human Computer interaction Computer\"\n",
    "new_vec=dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea83d799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)],\n",
      " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
      " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
      " [(1, 1), (5, 2), (8, 1)],\n",
      " [(3, 1), (6, 1), (7, 1)],\n",
      " [(9, 1)],\n",
      " [(9, 1), (10, 1), (11, 1)],\n",
      " [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "bow_corpus=[dictionary.doc2bow(text) for text in processed_corpus]\n",
    "\n",
    "# in each tuple below 1st occurance is ID and 2nd Ocurrance is count\n",
    "pprint.pprint(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa52e639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.5775744510957422), (11, 0.8163380142082396)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "# train model\n",
    "tfidf=models.TfidfModel(bow_corpus)\n",
    "\n",
    "# transform the \"system minors\" string\n",
    "words=\"system minors\".lower().split()\n",
    "\n",
    "print(tfidf[dictionary.doc2bow(words)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b050ec9",
   "metadata": {},
   "source": [
    "**Open text file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5ac6cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abc': 0, 'application': 1, 'computer': 2, 'for': 3, 'human': 4, 'interface': 5, 'lab': 6, 'machine': 7, 'of': 8, 'opinion': 9, 'response': 10, 'survey': 11, 'system': 12, 'time': 13, 'user': 14, 'eps': 15, 'management': 16, 'the': 17, 'and': 18, 'engineering': 19, 'testing': 20, 'error': 21, 'measurement': 22, 'perceived': 23, 'realtion': 24, 'to': 25, 'binary': 26, 'genration': 27, 'ordered': 28, 'random': 29, 'then': 30, 'trees': 31, 'un': 32, 'graph': 33, 'iv': 34, 'minors': 35, 'ordering': 36, 'quasi': 37, 'well': 38, 'widths': 39}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import os\n",
    "\n",
    "dict_STF=corpora.Dictionary(simple_preprocess(line) for line in open(r\"sample.txt\"))\n",
    "print(dict_STF.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dca32f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
